{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8556f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VISHN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e68af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa1770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472a1009",
   "metadata": {},
   "source": [
    "# Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e33cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-mnli\"\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6220413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device):\n",
    "    theme_classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=model_name,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return theme_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f521da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "theme_classifier = load_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4350d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list = [\"friendship\",\"hope\",\"sacrifice\",\"battle\",\"self development\",\"betrayal\",\"love\",\"dialogue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b380eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I gave him a right hook then a left jab',\n",
       " 'labels': ['battle',\n",
       "  'self development',\n",
       "  'hope',\n",
       "  'sacrifice',\n",
       "  'dialogue',\n",
       "  'betrayal',\n",
       "  'love',\n",
       "  'friendship'],\n",
       " 'scores': [0.9121254086494446,\n",
       "  0.47499966621398926,\n",
       "  0.0878182128071785,\n",
       "  0.04499955102801323,\n",
       "  0.020132720470428467,\n",
       "  0.012040308676660061,\n",
       "  0.004292338155210018,\n",
       "  0.0028172165621072054]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_classifier(\n",
    "    \"I gave him a right hook then a left jab\",\n",
    "    theme_list,\n",
    "    multi_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272218b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d789659",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8195d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/Subtitles/*.ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c78c4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Subtitles\\\\Naruto Season 1 - 01.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 02.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 03.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 04.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 05.ass']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33171b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[0],'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines = lines[27:]\n",
    "    lines =  [ \",\".join(line.split(',')[9:])  for line in lines ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd4182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox\\\\Nappeared with nine tails.\\n',\n",
       " 'With its powerful tails,\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e294cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [ line.replace('\\\\N',' ') for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "354377b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox appeared with nine tails.\\n',\n",
       " 'With its powerful tails,\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feeb96df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A long time ago, a powerful demon fox appeared with nine tails.\\n With its powerful tails,\\n it could smash mountains and create tidal waves.\\n A band of Ninjas rose to defend their village from attack.\\n We have to wait until the Fourth Hokage gets here!\\n We can't let it get any closer to our village!\\n One great Ninja was able to imprison the monster,\\n but died in the process.\\n This Ninja was known asâ€¦ the Fourth Hokage.\\n Naruto!\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c861100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(files[0].split('-')[-1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d68f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subtitles_dataset(dataset_path):\n",
    "    subtitles_paths = glob(dataset_path+'/*.ass')\n",
    "\n",
    "    scripts=[]\n",
    "    episode_num=[]\n",
    "\n",
    "    for path in subtitles_paths:\n",
    "\n",
    "        #Read Lines\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = lines[27:]\n",
    "            lines =  [ \",\".join(line.split(',')[9:])  for line in lines ]\n",
    "        \n",
    "        lines = [ line.replace('\\\\N',' ') for line in lines]\n",
    "        script = \" \".join(lines)\n",
    "\n",
    "        episode = int(path.split('-')[-1].split('.')[0].strip())\n",
    "\n",
    "        scripts.append(script)\n",
    "        episode_num.append(episode)\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"episode\":episode_num, \"script\":scripts })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b762d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"../data/Subtitles\"\n",
    "df = load_subtitles_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1518c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "2        3  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "3        4  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "4        5  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d20083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "160129be",
   "metadata": {},
   "source": [
    "\n",
    "# Run Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "script = df.iloc[0]['script']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A long time ago, a powerful demon fox appeared with nine tails.\n",
      " With its powerful tails,\n",
      " it could smash mountains and create tidal waves.\n",
      " A band of Ninjas rose to defend their village from attack.\n",
      " We have to wait until the Fourth Hokage gets here!\n",
      " We can't let it get any closer to our village!\n",
      " One great Ninja was able to imprison the monster,\n",
      " but died in the process.\n",
      " This Ninja was known as… the Fourth Hokage.\n",
      " Naruto!\n",
      " Why did you do such a thing?!\n",
      " You're really gonna get it this time!\n",
      " I don't care!\n",
      " You know your problem?\n",
      " You can't do the things I do!\n",
      " Only I can do this!\n",
      " I'm better than all of you! Believe it!\n",
      " There's a problem, sir!\n",
      " Lord Hokage!\n",
      " What is it?\n",
      " Did that Naruto do something again?\n",
      " Yes. He climbed onto the Mountainside Images…\n",
      " And he vandalized and graffitied all over them!\n",
      " Wait!\n",
      " Ha ha…\n",
      " Why should I?\n",
      " Hey, Naruto!\n",
      " How did you suddenly get here, lruka Sensei?\n",
      " The question is what are you doing here when you should be in class now?\n",
      " Now listen, Naruto.\n",
      " You failed the last graduation test and the one before that.\n",
      " This is no time to be goofing off, you fool!\n",
      " We will have a re-test on the Transformation Jutsu!\n",
      " Even those who already passed will take it!\n",
      " Whaaaat?!\n",
      " Sakura Haruno. Here I go…\n",
      " Transform!\n",
      " OK!\n",
      " I did it!\n",
      " Cha!\n",
      " Did you see that, Sasuke?\n",
      " Next, Sasuke Uchiha.\n",
      " Yes.\n",
      " O-OK.\n",
      " Next, Naruto Uzumaki.\n",
      " This is a real pain.\n",
      " And it's all your fault.\n",
      " Like I care!!\n",
      " OK…\n",
      " Good luck, Naruto…\n",
      " Transform!\n",
      " How was it?\n",
      " I call it the \"Sexy Jutsu\"!\n",
      " You fool! Stop making idiotic spells!\n",
      " Darn…\n",
      " Darn…\n",
      " I won't let you go home unless you clean that all up.\n",
      " I don't care…\n",
      " There's nobody home anyway.\n",
      " Naruto...\n",
      " What is it this time?\n",
      " What I meant was…\n",
      " If you clean up all that mess, I'll buy you ramen tonight.\n",
      " Huh?!\n",
      " Yes! I-I will finish it no time!\n",
      " Enter: Naruto Uzumaki!\n",
      " Naruto.\n",
      " Why did you vandalize those faces?\n",
      " Don't you know who the Hokage leaders are?\n",
      " Of course, I do!\n",
      " I know they earned the title Lord Hokage\n",
      " because they were the best Ninja of their time, right?\n",
      " Especially the Fourth Hokage was a hero\n",
      " who saved the village from the nine-tail demon fox.\n",
      " Then why did you do that?\n",
      " Because I'll become a Hokage myself.\n",
      " And I'll be the greatest Hokage of all time!\n",
      " So that everyone will finally learn to accept me!\n",
      " By the way, Sensei, I have a favor to ask.\n",
      " You want another bowl?\n",
      " Mmmm…No…\n",
      " Can I borrow that Leaf headband for a while?\n",
      " This?\n",
      " No no!\n",
      " This is worn only by those who have graduated from Ninja Academy.\n",
      " Tomorrow, you will…\n",
      " You're so mean!\n",
      " So that's why you took off your goggles…\n",
      " Humph... One more bowl please!\n",
      " We are now about to begin the graduation test.\n",
      " When your name is called, proceed to the next classroom.\n",
      " The test is on the Clone Jutsu.\n",
      " Oh no…\n",
      " Of all the…! That is my weakest Jutsu!\n",
      " But still… I will do it no matter what!\n",
      " Clone Jutsu!\n",
      " Disqualified!\n",
      " Iruka Sensei.\n",
      " His physical coordination and stamina are excellent.\n",
      " And he managed to come up with something.\n",
      " Isn't that enough for him to pass?\n",
      " Mizuki Sensei... All the others created three or more clones.\n",
      " Naruto created just one.\n",
      " And it's practically useless. I can't give him a passing mark.\n",
      " I 'm a Ninja now!\n",
      " You did well. That's my son.\n",
      " Congratulations for your graduation.\n",
      " I'll cook something good tonight!\n",
      " Look at that one.\n",
      " It's that boy. I hear he's the only one who failed.\n",
      " Serves him right.\n",
      " Imagine what would happen if he became a Ninja.\n",
      " Isn't that the boy who is actually…\n",
      " Hey! We're not supposed to talk about that.\n",
      " Iruka. We need to talk later.\n",
      " Yes, sir.\n",
      " Iruka Sensei isn't trying to be mean to you.\n",
      " Then why only me?\n",
      " He wants you to become strong from the bottom of his heart.\n",
      " You both don't have parents.\n",
      " But I really wanted to graduate.\n",
      " Heh... I guess I have no choice…\n",
      " I'll let you in on a big secret.\n",
      " Secret?\n",
      " Iruka.\n",
      " What is it, Lord Hokage?\n",
      " I know how you feel. But…\n",
      " Naruto also grew up without knowing the love of his parents…like you.\n",
      " Let me go!\n",
      " My mom and dad are still out there fighting!\n",
      " Wake up, Iruka Sensei!\n",
      " What's the matter?\n",
      " Come to Lord Hokage's immediately!\n",
      " I heard that Naruto… stole the Scroll of Sealing.\n",
      " The Scroll of Sealing?!\n",
      " Let's see…\n",
      " The first Jutsu is… Multi-Shadow Clone Jutsu?\n",
      " What?! Already a Jutsu I'm no good at?\n",
      " Lord Hokage! We can't forgive him!\n",
      " This is not just a prank!\n",
      " The Scroll is a dangerous item that the First Hokage sealed!\n",
      " Depending on its use…\n",
      " It will be a major disaster if it is taken out of the village!\n",
      " Yes. Bring Naruto here at once!\n",
      " Yes, sir!\n",
      " Where did you go…Naruto?\n",
      " I will tell everyone in the village about this and eliminate Naruto…\n",
      " Then the Scroll of Sealing will be mine!\n",
      " Hey you, Naruto!\n",
      " You found me..\n",
      " And I've only learned one Jutsu.\n",
      " He's been practicing the Jutsu…\n",
      " until he's become this exhausted and dirty…?\n",
      " Listen, listen! I'm gonna show you this amazing Jutsu!\n",
      " You're gonna let me graduate if I can do it!\n",
      " Isn't it true that I can graduate if I can do one of the Jutsu written here?\n",
      " Who told you that?\n",
      " Mizuki Sensei. He told me about this scroll, and this place…\n",
      " Mizuki did?!\n",
      " I'm impressed you found this place.\n",
      " I see now…how it is.\n",
      " Naruto, give me that scroll.\n",
      " Wait, wait… What's going on here?\n",
      " Naruto! Never give him that scroll!\n",
      " It is a dangerous object that contains forbidden Ninja Jutsu. It was sealed.\n",
      " Mizuki used you in order to get it for himself!\n",
      " W-Wha--?\n",
      " Naruto, Iruka is only afraid of you holding that scroll!\n",
      " Huh?\n",
      " What are you saying, Mizuki! Don't let him fool you, Naruto!\n",
      " I will tell you the truth.\n",
      " Idiot! Don't do that!\n",
      " After an incident 12 years ago, a rule was created.\n",
      " A rule?\n",
      " That is, Naruto, a rule everybody but you knows.\n",
      " Except me?! \t\t\t\t\tWhat is it?\n",
      " Stop it, Mizuki!\n",
      " The rule forbids anyone from revealing that you are actually the Demon Fox Spirit!\n",
      " Huh?\n",
      " You are actually the Demon, Nine-Tailed Fox Spirit,\n",
      " who killed Iruka's parents and destroyed our village!\n",
      " Stop it!\n",
      " Everyone has been deceiving you ever since.\n",
      " Didn't you find it strange?\n",
      " Why everyone hated you so much?\n",
      " No! No! No! No! No!\n",
      " Naruto…\n",
      " Nobody accepts you. That's why Iruka hates you so much!\n",
      " Iruka... Naruto grew up without the love of parents.\n",
      " Everyone avoids him like the plague after what happened.\n",
      " That's why he keeps misbehaving.\n",
      " It's the only way for him to get any attention or acknowledgement.\n",
      " He pretends to be tough, but inside he is really hurting.\n",
      " Die, Naruto!\n",
      " Naruto! \t\t\t\t\tGet down!\n",
      " Why…?\n",
      " Because you and I are the same.\n",
      " After my parents died,\n",
      " nobody paid attention to me or gave me any support.\n",
      " I wasn't a good student in school.\n",
      " I was the class clown… because I wanted people to notice me.\n",
      " I couldn't get noticed through excellence, so I kept doing stupid things.\n",
      " It was so hard.\n",
      " Isn't that right, Naruto?\n",
      " You felt so lonely…right? And you suffered inside, right?\n",
      " I'm sorry, Naruto….\n",
      " If I had been more responsible, maybe you wouldn't have suffered so much.\n",
      " Don't make me laugh!\n",
      " Iruka has always hated you, ever since you killed his parents!\n",
      " He's just saying all that to get the Scroll of Sealing back!\n",
      " Naruto!\n",
      " Narutoooooo!\n",
      " He is not the type of kid who will change his mind.\n",
      " He will take revenge against our village using that scroll!\n",
      " Didn't you see his eyes? Those are the eyes of a Demon Fox.\n",
      " No… Naruto…isn't…like that at all!\n",
      " All I want is to kill Naruto and get the scroll.\n",
      " I'll take care of you later!\n",
      " I-I won't let you…\n",
      " Well, well.\n",
      " Mizuki has a big mouth!\n",
      " Naruto feels worse than he's ever felt.\n",
      " He might unleash the power locked up inside him.\n",
      " The Scroll of Sealing is now with him.\n",
      " There's a slight chance he might actually release\n",
      " the Nine-Tailed Fox Spirit sealed inside him!\n",
      " If that happens…\n",
      " I've found him!\n",
      " Naruto!\n",
      " Everything that Mizuki said was a lie!\n",
      " Give me that scroll, quick! Mizuki is after the scroll!\n",
      " It can't be…\n",
      " Why is it, Naruto?\n",
      " How…\n",
      " did you know I wasn't Iruka…?\n",
      " Because I'm Iruka.\n",
      " I see.\n",
      " What's in it for you to protect the one who killed your family?\n",
      " I'm not gonna let a stupid idiot like you get that scroll!\n",
      " You're the idiot. Naruto is the same as me.\n",
      " Same?\n",
      " Anyone can do whatever he wants once he has the scroll.\n",
      " There is no way that that monster…\n",
      " that Fox Spirit, won't take advantage of the power of that scroll!\n",
      " You're right…\n",
      " I guess it was true all along!\n",
      " See, Iruka Sensei never really cared for me at all!\n",
      " ...if he was the Demon Fox Spirit.\n",
      " But Naruto is different!\n",
      " I know that he is an exceptional student.\n",
      " He works very hard,\n",
      " and he's single-minded and clumsy at the same time.\n",
      " No one accepts him, but he knows the meaning of human suffering.\n",
      " He is not the Demon Fox Spirit.\n",
      " He's Naruto Uzumaki of the Village Hidden in the Leaves!\n",
      " You are so gullible. \t\t\t\t\tlruka!\n",
      " I was gonna take you down later, but I have changed my mind.\n",
      " Die!\n",
      " I guess this is the end for me…\n",
      " Naruto?!\n",
      " You surprised me there, freak.\n",
      " If you ever lay a hand on Iruka Sensei, I'll kill you!\n",
      " Shut up! I can take care of a kid like you with a single blow!\n",
      " Why don't you try then? I'll strike you back a thousand-fold!\n",
      " Let's see you try! Show me what you can do, Demon Fox!\n",
      " Shadow Clone Jutsu!\n",
      " Naruto! You've…\n",
      " Those aren't just images but actual clones! That's an advanced Ninjutsu!\n",
      " What's this…?\n",
      " What's the matter? \t\t\t\t\tC'mon!\n",
      " Weren't you gonna get me with one blow? Here!\n",
      " In that case… I'll come to you.\n",
      " I kinda got carried away. lruka Sensei, are you okay?\n",
      " Yeah.\n",
      " He's really something.\n",
      " Maybe it is true.\n",
      " Maybe he will surpass all the Hokage leaders…\n",
      " Naruto, come over here. I'd like to give you something.\n",
      " Has anyone found Naruto yet?\n",
      " No.\n",
      " Darn, this is going to be bad…\n",
      " There's no need to worry anymore.\n",
      " Lord Hokage!\n",
      " He'll be back soon.\n",
      " Sensei, how much longer?\n",
      " OK, you may open your eyes now.\n",
      " Congratulations…on your graduation.\n",
      " In celebration, we'll have ramen tonight!\n",
      " Iruka Sensei!\n",
      " That hurts!\n",
      " Naruto…\n",
      " I was going to lecture to you...\n",
      " that the road gets more difficult now that you're a Ninja.\n",
      " But I guess I'll just wait to tell you that until we get to the ramen stand…\n",
      " W-What do you want, you little shrimp? Quit following me!\n",
      " You're smaller than me and\n",
      " you're saying that you're gonna become the Fifth Hokage?\n",
      " I don't care if you are the 3rd Hokage's grandson or not.\n",
      " It's not that easy to be a Hokage!\n",
      " If you want it that bad, you're gonna have to beat me first!\n",
      " Next episode:  \"My Name Is Konohamaru!\"\n",
      " Watch my outstanding performance!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "script\n",
    "print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4bf2bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A long time ago, a powerful demon fox appeared with nine tails.', 'With its powerful tails,\\n it could smash mountains and create tidal waves.', 'A band of Ninjas rose to defend their village from attack.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "script = df.iloc[0]['script']\n",
    "\n",
    "# Split on '.', '!', or '?' followed by a space or end of string\n",
    "script_sentences = re.split(r'(?<=[.!?])\\s+', script)\n",
    "\n",
    "print(script_sentences[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4928bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Sentence\n",
    "sentence_batch_size=20\n",
    "script_batches = []\n",
    "for index in range(0,len(script_sentences),sentence_batch_size):\n",
    "    sent = \" \".join(script_sentences[index:index+sentence_batch_size])\n",
    "    script_batches.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80426999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails,\\n it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster,\\n but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       " 'Yes. He climbed onto the Mountainside Images…\\n And he vandalized and graffitied all over them! Wait! Ha ha…\\n Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go…\\n Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "script_batches[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee24e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_output = theme_classifier(\n",
    "    script_batches[:2],\n",
    "    theme_list,\n",
    "    multi_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39238826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails,\\n it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster,\\n but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       "  'labels': ['dialogue',\n",
       "   'betrayal',\n",
       "   'battle',\n",
       "   'sacrifice',\n",
       "   'self development',\n",
       "   'hope',\n",
       "   'friendship',\n",
       "   'love'],\n",
       "  'scores': [0.9800736904144287,\n",
       "   0.9396899938583374,\n",
       "   0.8546878099441528,\n",
       "   0.7349792122840881,\n",
       "   0.7284933924674988,\n",
       "   0.19909851253032684,\n",
       "   0.05922350287437439,\n",
       "   0.04026192054152489]},\n",
       " {'sequence': 'Yes. He climbed onto the Mountainside Images…\\n And he vandalized and graffitied all over them! Wait! Ha ha…\\n Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go…\\n Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.',\n",
       "  'labels': ['dialogue',\n",
       "   'self development',\n",
       "   'battle',\n",
       "   'betrayal',\n",
       "   'sacrifice',\n",
       "   'hope',\n",
       "   'friendship',\n",
       "   'love'],\n",
       "  'scores': [0.9370126128196716,\n",
       "   0.8678195476531982,\n",
       "   0.6581308841705322,\n",
       "   0.645724356174469,\n",
       "   0.6258835196495056,\n",
       "   0.20424096286296844,\n",
       "   0.08603237569332123,\n",
       "   0.02802066132426262]}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c653e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wrangle Ouput\n",
    "# battle: [0.51489498, 0.2156498]\n",
    "themes = {}\n",
    "for output in theme_output:\n",
    "    for label,score in zip(output['labels'],output['scores']):\n",
    "        if label not in themes:\n",
    "            themes[label] = []\n",
    "        themes[label].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "671f841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "themes = {key: np.mean(np.array(value)) for key,value in themes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f8e2273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': np.float64(0.9585431516170502),\n",
       " 'betrayal': np.float64(0.7927071750164032),\n",
       " 'battle': np.float64(0.7564093470573425),\n",
       " 'sacrifice': np.float64(0.6804313659667969),\n",
       " 'self development': np.float64(0.7981564700603485),\n",
       " 'hope': np.float64(0.20166973769664764),\n",
       " 'friendship': np.float64(0.07262793928384781),\n",
       " 'love': np.float64(0.03414129093289375)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_themes_inference(script):\n",
    "    script_sentences = sent_tokenize(script)\n",
    "\n",
    "    # Batch Sentence\n",
    "    sentence_batch_size=20\n",
    "    script_batches = []\n",
    "    for index in range(0,len(script_sentences),sentence_batch_size):\n",
    "        sent = \" \".join(script_sentences[index:index+sentence_batch_size])\n",
    "        script_batches.append(sent)\n",
    "    \n",
    "    # Run Model\n",
    "    theme_output = theme_classifier(\n",
    "        script_batches[:2],\n",
    "        theme_list,\n",
    "        multi_label=True\n",
    "    )\n",
    "\n",
    "    # Wrangle Output \n",
    "    themes = {}\n",
    "    for output in theme_output:\n",
    "        for label,score in zip(output['labels'],output['scores']):\n",
    "            if label not in themes:\n",
    "                themes[label] = []\n",
    "            themes[label].append(score)\n",
    "\n",
    "    themes = {key: np.mean(np.array(value)) for key,value in themes.items()}\n",
    "\n",
    "    return themes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7561f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11a658f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5cea7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_themes = df['script'].apply(get_themes_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e60c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # or whichever resource is missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f34bf6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\VISHN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')  # downloads everything (large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'dialogue': 0.9585431516170502, 'betrayal': 0...\n",
       "1    {'dialogue': 0.9606050848960876, 'sacrifice': ...\n",
       "Name: script, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d3cf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_df = pd.DataFrame(output_themes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>battle</th>\n",
       "      <th>sacrifice</th>\n",
       "      <th>self development</th>\n",
       "      <th>hope</th>\n",
       "      <th>friendship</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.792707</td>\n",
       "      <td>0.756409</td>\n",
       "      <td>0.680431</td>\n",
       "      <td>0.798156</td>\n",
       "      <td>0.201670</td>\n",
       "      <td>0.072628</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.960605</td>\n",
       "      <td>0.429944</td>\n",
       "      <td>0.684843</td>\n",
       "      <td>0.570703</td>\n",
       "      <td>0.482808</td>\n",
       "      <td>0.154534</td>\n",
       "      <td>0.046261</td>\n",
       "      <td>0.173262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialogue  betrayal    battle  sacrifice  self development      hope  \\\n",
       "0  0.958543  0.792707  0.756409   0.680431          0.798156  0.201670   \n",
       "1  0.960605  0.429944  0.684843   0.570703          0.482808  0.154534   \n",
       "\n",
       "   friendship      love  \n",
       "0    0.072628  0.034141  \n",
       "1    0.046261  0.173262  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "theme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf7dd01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8eb099a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>battle</th>\n",
       "      <th>sacrifice</th>\n",
       "      <th>self development</th>\n",
       "      <th>hope</th>\n",
       "      <th>friendship</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.792707</td>\n",
       "      <td>0.756409</td>\n",
       "      <td>0.680431</td>\n",
       "      <td>0.798156</td>\n",
       "      <td>0.201670</td>\n",
       "      <td>0.072628</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "      <td>0.960605</td>\n",
       "      <td>0.429944</td>\n",
       "      <td>0.684843</td>\n",
       "      <td>0.570703</td>\n",
       "      <td>0.482808</td>\n",
       "      <td>0.154534</td>\n",
       "      <td>0.046261</td>\n",
       "      <td>0.173262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script  dialogue  \\\n",
       "0        1  A long time ago, a powerful demon fox appeared...  0.958543   \n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...  0.960605   \n",
       "\n",
       "   betrayal    battle  sacrifice  self development      hope  friendship  \\\n",
       "0  0.792707  0.756409   0.680431          0.798156  0.201670    0.072628   \n",
       "1  0.429944  0.684843   0.570703          0.482808  0.154534    0.046261   \n",
       "\n",
       "       love  \n",
       "0  0.034141  \n",
       "1  0.173262  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[theme_df.columns] = theme_df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e602db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f27933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eba06f7",
   "metadata": {},
   "source": [
    "# Visualize output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72fc2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('dialogue', axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f299d768",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['episode'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m theme_output = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepisode\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscript\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.sum().reset_index()\n\u001b[32m      2\u001b[39m theme_output.columns = [\u001b[33m'\u001b[39m\u001b[33mtheme\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m theme_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VISHN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VISHN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VISHN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4828\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4830\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4831\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4833\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4834\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VISHN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['episode'] not found in axis\""
     ]
    }
   ],
   "source": [
    "theme_output = df.drop(['episode','script'],axis=1).sum().reset_index()\n",
    "theme_output.columns = ['theme','score']\n",
    "theme_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5281db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type        Data/Info\n",
      "-------------------------------------\n",
      "glob            function    <function glob at 0x0000028C1481CA40>\n",
      "nltk            module      <module 'nltk' from 'c:\\\\<...>ages\\\\nltk\\\\__init__.py'>\n",
      "np              module      <module 'numpy' from 'c:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "pd              module      <module 'pandas' from 'c:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pipeline        function    <function pipeline at 0x0000028C35C9EB60>\n",
      "sent_tokenize   function    <function sent_tokenize at 0x0000028C3754ACA0>\n",
      "torch           module      <module 'torch' from 'c:\\<...>ges\\\\torch\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bcb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\VISHN/nltk_data', 'c:\\\\Users\\\\VISHN\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data', 'c:\\\\Users\\\\VISHN\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data', 'c:\\\\Users\\\\VISHN\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\VISHN\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'nltk_data', 'nltk_data', 'nltk_data', 'nltk_data']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
